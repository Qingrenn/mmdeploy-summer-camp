# Week2

## 1. è®ºæ–‡é˜…è¯»

- [*Quantization and training of neural networks for efficient integer-arithmetic-only inference*](https://openaccess.thecvf.com/content_cvpr_2018/html/Jacob_Quantization_and_Training_CVPR_2018_paper.html): [ğŸ“ç¬”è®°](paper1.md)

- [*EasyQuant: Post-training Quantization via Scale Optimization*](https://arxiv.org/abs/2006.16669): [ğŸ“ç¬”è®°](paper2.md)

---

## 2. å¯¹ç§°é‡åŒ–å’Œéå¯¹ç§°é‡åŒ–

---

## 3. ncnnçš„é‡åŒ–æ–¹æ¡ˆ

å‚è€ƒï¼š[çŸ¥ä¹ï¼šMegFlowå’Œncnn int8ç®€ä»‹](https://zhuanlan.zhihu.com/p/476605320)

ncnnæ‰€é‡‡ç”¨çš„æ˜¯ä¸€ç§åè®­ç»ƒçš„å¯¹ç§°é‡åŒ–æ–¹æ¡ˆï¼Œå…·ä½“å®ç°è§[ncnn2table](https://github.com/Tencent/ncnn/blob/master/tools/quantize/ncnn2table.cpp)ã€‚

### 3.1é‡åŒ–æœºåˆ¶

$q = clamp(round(r*S))$

å…¶ä¸­$q$æ˜¯é‡åŒ–åçš„å‚æ•°ï¼Œä¸ºint8ç±»å‹ï¼›$r$ä¸ºæƒé‡æˆ–æ¿€æ´»å€¼çš„çœŸå®å€¼ï¼Œä¸ºfpç±»å‹ï¼›$S$ä¸ºé‡åŒ–å‚æ•°-ç¼©æ”¾ç³»æ•°ï¼Œæ¯ä¸ªå·ç§¯æ ¸æˆ–æ¯å±‚çš„æ¿€æ´»å€¼éƒ½å…·æœ‰ä¸€ä¸ªä¸ä¹‹å¯¹åº”çš„é‡åŒ–å‚æ•°ã€‚

### 3.2ç¡®å®šé‡åŒ–å‚æ•°

ä»¥â€œklâ€é‡åŒ–æ–¹æ³•ä¸ºä¾‹ï¼Œä»‹ç»é‡åŒ–å‚æ•°æ˜¯å¦‚ä½•ç¡®å®šçš„ã€‚

**1.ç¡®å®šæ¨¡å‹æƒé‡çš„é‡åŒ–å‚æ•°**

$S = 127 / absmax(W)$

å…¶ä¸­$absmax(W)$æ˜¯å·ç§¯æ ¸æƒé‡$W$ä¸­çš„æœ€å¤§ç»å¯¹å€¼ã€‚

**2.ç¡®å®šå„å±‚æ¿€æ´»å€¼ï¼ˆç‰¹å¾å›¾ï¼‰çš„é‡åŒ–å‚æ•°**

<left><img src="images/img1.png" width="80%"></left>

ä¸Šå›¾æè¿°äº†ç¡®å®šæ¿€æ´»å€¼çš„é‡åŒ–å‚æ•°çš„è¿‡ç¨‹ã€‚
1. å‡†å¤‡çŸ«æ­£é›†ï¼Œå°†å…¶é€å…¥æ¨¡å‹ï¼Œç”¨äºè·å¾—å„å±‚æ¿€æ´»å€¼çš„åˆ†å¸ƒã€‚
2. ç»Ÿè®¡æ¯ä¸€å±‚æ¿€æ´»å€¼ä¸­çš„æœ€å¤§ç»å¯¹å€¼måŠå…¶åˆ†å¸ƒã€‚ï¼ˆç»Ÿè®¡æ¿€æ´»å€¼åˆ†å¸ƒæ—¶è®¾ç½®äº†2048ä¸ªç­‰é—´éš”çš„bin, é—´éš”å¤§å°ä¸ºst0=2048/mï¼‰
3. è®¾ç½®ä¸€ä¸ªé˜ˆå€¼$t \in [128,2048)$ã€‚
    - è®¡ç®—è£åˆ‡åçš„åˆ†å¸ƒ`clip_distribution`: è®¾ç½®tä¸ªbinsï¼Œå°†2048ä¸ªbinsä¸­ç´¢å¼•å¤§äºtçš„binä¸­çš„å…ƒç´ ï¼Œå…¨éƒ¨ç»Ÿè®¡è‡³ç¬¬tä¸ªbinä¸­ã€‚å¦‚ä¸Šå›¾ä¸­éƒ¨æ‰€ç¤ºã€‚
    - è®¡ç®—é‡åŒ–åçš„åˆ†å¸ƒ`quantize_distribution`ï¼šè®¾ç½®128ä¸ªbinsï¼Œæ¯ä¸ªbinså¯¹åº”çš„é—´éš”st1=st0*(t/128)ã€‚ä¹Ÿå°±æ˜¯å°†2048ä¸ªbinsä¸­çš„å‰tä¸ªbinsçš„æ•°æ®é‡æ–°åˆ’åˆ†è‡³128ä¸ªbinsä¸­ã€‚
    - å¯¹é‡åŒ–åçš„åˆ†å¸ƒè¿›è¡Œæ‰©å¼ `expand_distribution`ï¼šå°†128ä¸ªbinsçš„æ•°æ®æ‰©å¼ è‡³tä¸ªbinsä¸­ã€‚
    - åº¦é‡`clip_distribution`å’Œ`expand_distribution`ä¸¤ä¸ªåˆ†å¸ƒä¹‹é—´çš„KLæ•£åº¦ã€‚
4. é‡å¤ç¬¬ä¸‰æ­¥çš„æ“ä½œï¼Œæ‰¾åˆ°ä¸€ä¸ªåˆé€‚çš„é˜ˆå€¼tï¼Œä½¿å¾—KLæ•£åº¦æœ€å°ï¼Œå³`clip_distribution`å’Œ`expand_distribution`æœ€æ¥è¿‘ã€‚
5. å¯¹é˜ˆå€¼tåšä¸€ä¸ªæ˜ å°„ï¼Œå°†å…¶ä»binçš„ç´¢å¼•æ˜ å°„è‡³çœŸå®å€¼ã€‚çœŸå®çš„é˜ˆå€¼ä¸º$t'=(t+0.5)/2048*m$
6. ç¡®å®šç¼©æ”¾å°ºåº¦$S=127/t'$

ç»è¿‡æ­¤è¿‡ç¨‹ï¼Œå³å¯ç”Ÿæˆä¸€ä¸ªé‡åŒ–å‚æ•°è¡¨ã€‚ä¾‹å¦‚week1é‡åŒ–squeezenetä»»åŠ¡ä¸­æ‰€ç”Ÿæˆçš„[squeezenet_v1.1.table](week1/Quantize-squeezenet/squeezenet_v1.1.table)ã€‚

ä¸Šè¿°è¿‡ç¨‹çš„[Pythonå®ç°](https://github.com/BUG1989/caffe-int8-convert-tools/blob/93ec69e465252e2fb15b1fc8edde4a51c9e79dbf/caffe-int8-convert-tool-dev-weight.py#L483)å’ŒC++å®ç°æ˜¯åŒä¸€ä¸ªå¤§ä½¬å†™çš„ã€‚

### 3.3 å°†æ¨¡å‹è½¬æ¢è‡³int8

[ncnn2int8](https://github.com/Tencent/ncnn/blob/master/tools/quantize/ncnn2int8.cpp)å®ç°äº†ä»¥ä¸‹åŠŸèƒ½ï¼š
- å°†æ¨¡å‹æƒé‡é‡åŒ–æˆint8ã€‚
- æ ¹æ®é‡åŒ–å‚æ•°è¡¨ï¼Œå°†é‡åŒ–å‚æ•°ç»‘å®šè‡³å„å±‚ä¸­ã€‚
- å°†åé‡åŒ–æ“ä½œå’Œé‡é‡åŒ–æ“ä½œèåˆæˆä¸€ä¸ªæ“ä½œã€‚å¦‚ä¸‹å›¾æ‰€ç¤ºã€‚

<left><img src="images/img2.png" width="80%"></left>

ç»è¿‡ä¸Šè¿°è¿‡ç¨‹ï¼Œä¼šç”Ÿæˆé‡åŒ–åçš„æ¨¡å‹æƒé‡ã€‚ä¾‹å¦‚week1é‡åŒ–squeezenetä»»åŠ¡ä¸­æ‰€ç”Ÿæˆçš„`squeezenet_v1.1-int8.bin`å’Œ`squeezenet_v1.1-int8.param`ã€‚


## 4. ncnn-int8 çš„å·ç§¯å®ç°

å…·ä½“å®ç°è§[æºç ](https://github.com/Tencent/ncnn/blob/master/src/layer/convolution.cpp)ã€‚

å¯¹äºä¸€ä¸ªå·ç§¯å±‚æ¥è¯´ï¼Œå…¶å¯¹åº”çš„é‡åŒ–å‚æ•°å¦‚ä¸‹æ‰€ç¤ºï¼Œå…¶ä¸­`weight_data_int8_scales`æ˜¯å·ç§¯æƒé‡å¯¹åº”çš„é‡åŒ–å‚æ•°ï¼Œè€Œ`bottom_blob_int8_scales`å’Œ`bottom_blob_int8_scales`åˆ†åˆ«å¯¹åº”å·ç§¯è¾“å…¥ç‰¹å¾å›¾å¯¹åº”å’Œè¾“å‡ºç‰¹å¾å›¾å¯¹åº”çš„å‚æ•°ã€‚

```C++
// Convolution class
#if NCNN_INT8
    Mat weight_data_int8_scales;
    Mat bottom_blob_int8_scales;
    Mat top_blob_int8_scales;
#endif
```

å…¶ä¸­`weight_data_int8_scales`æ˜¯æ¨¡å‹æƒé‡å¯¹åº”çš„ç¼©æ”¾å› å­ã€‚

å…¶ä¸­`bottom_blob_int8_scales`æ˜¯å½“å‰å·ç§¯å±‚è¾“å…¥ç‰¹å¾å›¾å¯¹åº”çš„ç¼©æ”¾å› å­ã€‚

è€Œ`top_blob_int8_scales`åˆ™å½“å‰å·ç§¯å±‚çš„ä¸‹ä¸€ä¸ªå·ç§¯ï¼ˆå…¨è¿æ¥ï¼‰å±‚è¾“å…¥ç‰¹å¾å›¾å¯¹åº”çš„ç¼©æ”¾å› å­ã€‚

forward_int8çš„æµç¨‹:

1. åŸºäºinput_scaleï¼Œå¯¹è¾“å…¥çš„ç‰¹å¾å›¾è¿›è¡Œé‡åŒ–ã€‚
2. å¯¹äºä¸€ä¸ªå·ç§¯æ ¸ä½œä¸€æ¬¡å·ç§¯è¾“å‡ºçš„çš„int32ç±»å‹çš„ç´¯åŠ å’Œsumï¼Œè¿›è¡Œåé‡åŒ–ï¼Œå³ sumfp32 = sum / (weight_scales * input_scale)ã€‚
3. åœ¨sumfp32çš„åŸºç¡€ä¸ŠåŠ ä¸Šbiasï¼Œé€šè¿‡æ¿€æ´»å‡½æ•°ã€‚
4. å¯¹è¾“å‡ºçš„æ¿€æ´»å€¼é‡é‡åŒ–ï¼šsumfp32 * scale_outã€‚è¿™ä¸€æ­¥åŸºäºçš„æ¨¡å‹ç»“æ„ï¼Œä¹Ÿå¯èƒ½ä¸åšï¼Œå¦‚3.3å›¾æ‰€ç¤ºã€‚
5. é‡å¤ä¸Šè¿°çš„2ï¼Œ3ï¼Œ4æ­¥ç›´åˆ°å®Œæˆæ•´ä¸ªå·ç§¯æ“ä½œã€‚

input_scale=`bottom_blob_int8_scales`

weight_scales=`bottom_blob_int8_scales`

scale_out=`top_blob_int8_scales`
